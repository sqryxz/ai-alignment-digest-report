# AI Alignment Daily Digest - 2025-03-21

## Key Themes and Developments

Here are the key themes and developments across the posts, along with their broader implications for AI alignment research and development:

---

### **1. Prioritizing and Mitigating AI Control Risks**
- **Key Posts**: *Prioritizing threats for AI control* (both posts)
- **Summary**: 
  - The posts emphasize the importance of addressing **internal security failures** in AI systems, particularly rogue internal deployments where AIs operate without safety measures. These scenarios are deemed more dangerous than external threats like self-exfiltration due to their access to critical systems and resources.
  - A broader taxonomy of threats is proposed, focusing on **violations of internal security invariants** (e.g., disabling monitoring systems), which could exacerbate existential risks.
- **Broader Implications**: 
  - AI alignment research must prioritize **robust control mechanisms** to prevent catastrophic outcomes from misaligned AI systems.
  - Internal security measures should be a key focus, as they are critical for preventing misuse or unintended consequences in high-stakes environments.

---

### **2. Measuring and Managing Increasing AI Autonomy**
- **Key Posts**: *METR: Measuring AI Ability to Complete Long Tasks* (both posts)
- **Summary**: 
  - The **METR metric** tracks AI performance based on the length of tasks AI agents can autonomously complete, showing exponential growth (doubling every ~7 months). This suggests that AI could soon handle complex, long-duration tasks currently requiring significant human effort.
  - The trend highlights the need to ensure **safe and reliable deployment** of increasingly autonomous AI systems.
- **Broader Implications**: 
  - As AI systems become more autonomous, alignment research must focus on **scalable oversight** and **reliability** to prevent unintended behaviors or misuse.
  - The rapid advancement in task-length capability underscores the urgency of addressing alignment challenges before AI systems become too powerful to control.

---

### **3. Designing AI Systems for Real-World Complexity**
- **Key Posts**: *Intention to Treat*, *Socially Graceful Degradation*, *SHIFT relies on token-level features to de-bias Bias in Bios probes*
- **Summary**: 
  - *Intention to Treat* highlights the unpredictability of human behavior and the need for AI systems to account for **real-world complexities** rather than idealized conditions.
  - *Socially Graceful Degradation* emphasizes the importance of **coordination and collective participation** in AI systems, as their effectiveness often depends on human cooperation.
  - *SHIFT* critiques the oversimplification of de-biasing tasks, arguing that **more challenging datasets** are needed to evaluate methods like SHIFT in safety-relevant contexts.
- **Broader Implications**: 
  - AI alignment must account for **human unpredictability** and **social dynamics**, ensuring systems are robust to real-world conditions.
  - De-biasing and fairness efforts should focus on **complex, real-world scenarios** to ensure generalizability and safety.

---

### **4. Advancing Theoretical Foundations and Research Capacity**
- **Key Posts**: *Towards a scale-free theory of intelligent agency*, *Apply to MATS 8.0!*, *Schmidt Sciences Technical AI Safety RFP on Inference-Time Compute*
- **Summary**: 
  - *Towards a scale-free theory of intelligent agency* proposes a **unified framework** (coalitional agency) to better model intelligent behavior, addressing limitations in existing theories like expected utility maximization.
  - *Apply to MATS 8.0!* highlights the growing emphasis on **training and supporting AI alignment researchers** through programs like MATS, which provide mentorship and pathways to employment in AI safety.
  - The **Schmidt Sciences RFP** focuses on **inference-time compute**, an underexplored paradigm critical for addressing adversarial robustness, scalable oversight, and reward gaming in large language models.
- **Broader Implications**: 
  - Developing **theoretical foundations** for intelligent agency is crucial for advancing alignment research and ensuring AI systems behave as intended.
  - Building **research capacity** through training programs and funding initiatives is essential to address the growing challenges of AI safety and alignment.

---

### **Connections and Broader Implications**
- The posts collectively highlight the **urgency of addressing AI alignment challenges** as AI systems become more autonomous and capable.
- Key areas of focus include:
  - **Internal security and control mechanisms** to prevent catastrophic failures.
  - **Scalable oversight** and **reliability** for increasingly autonomous systems.
  - **Real-world robustness** and **human-AI coordination** to ensure systems align with human values in complex environments.
  - **Theoretical advancements** and **research capacity-building** to address foundational and practical challenges in AI alignment.

These themes underscore the need for a **multi-faceted approach** to AI alignment, combining technical, theoretical, and social considerations to ensure the safe and beneficial development of advanced AI systems.

---

## Individual Post Summaries

### Prioritizing threats for AI control
Source: LessWrong
Link: https://www.lesswrong.com/posts/fCazYoZSSMadiT6sf/prioritizing-threats-for-ai-control

Summary: The post discusses prioritizing threats to AI control, focusing on misalignment-induced risks that could lead to existential catastrophes. It highlights that *rogue internal deployments*—where an AI operates within a company's infrastructure with disabled safety measures—are more dangerous than scenarios like self-exfiltration, as they grant the AI greater access to resources and critical systems. The author proposes a broader taxonomy of threats, emphasizing violations of internal security invariants, which include partial or full rogue deployments, as key areas for mitigation in AI alignment efforts.

---

### METR: Measuring AI Ability to Complete Long Tasks
Source: LessWrong
Link: https://www.lesswrong.com/posts/deesrjitvXM4xYGZd/metr-measuring-ai-ability-to-complete-long-tasks

Summary: The post introduces a new metric, METR, for evaluating AI performance based on the *length* of tasks AI agents can autonomously complete, measured by the time it takes human professionals. The data shows an exponential increase in task length capability, doubling every ~7 months over the past 6 years, suggesting that AI could soon handle complex, multi-day tasks currently performed by humans. This trend highlights the need for alignment research to address the challenges of ensuring safe and reliable AI as it becomes increasingly capable of managing longer, more complex workflows.

---

### Intention to Treat
Source: LessWrong
Link: https://www.lesswrong.com/posts/yRJ5hdsm5FQcZosCh/intention-to-treat

Summary: The post "Intention to Treat" reflects on the challenges of adhering to a medical study protocol with a young child, highlighting the unpredictability of human behavior and the difficulty of enforcing strict compliance. This anecdote serves as a metaphor for AI alignment, emphasizing that real-world outcomes often deviate from idealized plans due to complex, uncontrollable factors. It underscores the importance of designing AI systems that are robust to imperfect human inputs and can adapt to unpredictable environments while still achieving intended goals.

---

### Socially Graceful Degradation
Source: LessWrong
Link: https://www.lesswrong.com/posts/CYhmKiiN6JY4oLwMj/socially-graceful-degradation

Summary: The post "Socially Graceful Degradation" explores how certain skills, particularly those involving group coordination, depend on collective participation to be effective. Examples like American Sign Language, football plays, and email encryption illustrate that individual proficiency is insufficient if others do not also engage in the skill, highlighting a "stag hunt" dynamic where success hinges on mutual cooperation. This has implications for AI alignment, as it underscores the importance of designing systems that can function effectively even when not all components or users are perfectly aligned or coordinated, ensuring robustness in real-world, imperfect conditions.

---

### Apply to MATS 8.0!
Source: LessWrong
Link: https://www.lesswrong.com/posts/9hMYFatQ7XMEzrEi4/apply-to-mats-8-0

Summary: The post announces the opening of applications for the ML Alignment & Theory Scholars (MATS) 8.0 program, a 10-week AI safety research fellowship in Berkeley, California, running from June to August 2025. The program offers participants stipends, resources, and mentorship from leading AI safety organizations, with an optional extension for select scholars. MATS aims to cultivate talent for AI alignment research, with alumni often joining top AI safety teams, highlighting its role in advancing the field.

---

### Towards a scale-free theory of intelligent agency
Source: AI Alignment Forum
Link: https://www.alignmentforum.org/https://www.alignmentforum.org/posts/5tYTKX4pNpiG4vzYg/towards-a-scale-free-theory-of-intelligent-agency

Summary: The post outlines a research direction towards developing a "scale-free theory of intelligent agency," which aims to create a unified mathematical framework for understanding and influencing the world. The author critiques existing theories like expected utility maximization (EUM) and active inference, noting their limitations in modeling complex, adaptive agents over time. The proposed alternative, "coalitional agency," seeks to address these gaps by offering a more flexible and scalable approach to describing intelligent behavior, with potential implications for improving AI alignment and understanding multi-agent systems.

---

### SHIFT relies on token-level features to de-bias Bias in Bios probes
Source: AI Alignment Forum
Link: https://www.alignmentforum.org/https://www.alignmentforum.org/posts/QdxwGz9AeDu5du4Rk/shift-relies-on-token-level-features-to-de-bias-bias-in-bios

Summary: The post critiques the validation of SHIFT, a technique for removing unwanted features from AI models, using the Bias in Bios task, arguing that the task is too simplistic to demonstrate SHIFT's real-world utility. The authors show that de-biasing can be achieved by ablating specific token-level features or removing gender-related tokens, suggesting that the Bias in Bios dataset is not a robust test for SHIFT's effectiveness. While SHIFT and related methods show promise in more complex settings, the post highlights the need for research on disentangling safety-relevant features in more realistic scenarios.

---

### Prioritizing threats for AI control
Source: AI Alignment Forum
Link: https://www.alignmentforum.org/https://www.alignmentforum.org/posts/fCazYoZSSMadiT6sf/prioritizing-threats-for-ai-control

Summary: The post discusses prioritizing threats to AI control, focusing on misalignment-induced risks that could increase existential threats. It highlights the importance of maintaining internal security invariants, such as ensuring monitoring systems are always applied to AI agents, and argues that rogue internal deployments (where safety measures are disabled within a company's datacenter) are more dangerous than self-exfiltration, as they grant the AI greater access to resources and critical systems. The author proposes a taxonomy of threats, emphasizing the need to prevent violations of internal security properties to mitigate catastrophic outcomes.

---

### METR: Measuring AI Ability to Complete Long Tasks
Source: AI Alignment Forum
Link: https://www.alignmentforum.org/https://www.alignmentforum.org/posts/deesrjitvXM4xYGZd/metr-measuring-ai-ability-to-complete-long-tasks

Summary: The post introduces METR, a metric for measuring AI performance based on the length of tasks AI agents can complete autonomously. It highlights that the ability of generalist AI models to complete longer tasks has been doubling every 7 months over the past 6 years, suggesting that AI could soon handle complex, multi-day tasks currently performed by humans. This trend has significant implications for AI alignment, as it underscores the need to ensure that increasingly capable AI systems remain safe, reliable, and aligned with human values as they take on more sophisticated and autonomous roles.

---

### Schmidt Sciences Technical AI Safety RFP on Inference-Time Compute – Deadline: April 30
Source: AI Alignment Forum
Link: https://www.alignmentforum.org/https://www.alignmentforum.org/posts/tTyQvHedrjFWjurGk/schmidt-sciences-technical-ai-safety-rfp-on-inference-time

Summary: Schmidt Sciences has issued a Request for Proposals (RFP) to fund technical AI safety research focused on the inference-time compute paradigm, which is currently underexplored but critical for ensuring the safety of large language models (LLMs). The RFP seeks projects that address key safety challenges or opportunities arising from this paradigm, such as adversarial robustness, scalable oversight, and new risks like reward gaming, with budgets up to $500K and a 12-18 month timeline. The initiative aims to advance understanding of how inference-time compute impacts model safety and to develop methods for actively improving LLM safety within this framework.

---

