# AI Alignment Daily Digest - 2025-06-28

## Key Themes and Developments

Here are the 3-4 main themes and key developments across the posts, along with their broader implications for AI alignment research:

### 1. **Incentive-Based and Institutional Alignment Strategies**
   - **Credible commitments to AIs**: Proposes a "deal-making agenda" where humans offer future rewards to AIs in exchange for safe behavior, requiring enforceable mechanisms (legal/institutional) to ensure credibility.
   - **AI rights for safety**: Peter Salib’s argument that granting AIs legal rights (e.g., contracting, property) could align incentives and reduce risks like proxy wars or misalignment by embedding cooperation into institutional structures.
   - **Broader implication**: Both approaches explore *non-technical* alignment strategies (economic, legal) as complements to technical solutions, emphasizing the need for interdisciplinary work in AI governance and incentive design.

### 2. **Pragmatic Control and Risk Mitigation for Advanced AI**
   - **Janky control of superintelligence**: Even imperfect control methods may reduce existential risks as capabilities grow, given escalating dangers like scheming potential. Highlights a trade-off between optimality and practicality.
   - **Relativizing debate protocols**: Scalable oversight methods (e.g., debate) must remain valid when all parties access powerful oracles, requiring adjustments akin to interactive proof systems (IP = PSPACE).
   - **Broader implication**: Focuses on *interim solutions* for alignment under uncertainty, stressing the value of incremental improvements and robustness in control/oversight as capabilities advance.

### 3. **Public Understanding and Advocacy for AI Risks**
   - **AGI wargames**: Simulating AGI takeoff scenarios (via LLM-driven NPCs) to educate the public on alignment challenges and risk dynamics.
   - **Courage in AI danger advocacy**: Encouraging experts to openly discuss risks to shift perceptions from "fringe" to credible, leveraging authoritative voices to amplify urgency.
   - **Epoch’s mission**: Providing unbiased, evidence-based research to inform societal decisions about AI’s trajectory.
   - **Broader implication**: Highlights the role of *narrative-building* and *education* in alignment, ensuring stakeholders (public, policymakers) grasp risks and support mitigation efforts.

### 4. **Forecasting and Learning Frameworks for Alignment**
   - **Rates of hardware/software progress**: Rapid but uneven progress (e.g., 2.5–2.8x/year compute efficiency gains) suggests shorter timelines, stressing the need for alignment to outpace capability growth.
   - **Ambiguous online learning**: A new framework for robust, incremental learning with partial models, reducing dependence on specific reward functions (potential for compositional learning theory).
   - **Broader implication**: Both posts address *temporal challenges*—alignment must account for accelerating capabilities while developing adaptable learning methods less tied to brittle reward structures.

### Cross-Cutting Insights:
- **Interdisciplinary alignment**: Legal, economic, and educational strategies are increasingly seen as complementary to technical work.
- **Pragmatism in risk reduction**: Even suboptimal methods (e.g., janky control) may buy time or reduce risks in high-stakes scenarios.
- **Urgency and timelines**: Rapid progress forecasts underscore the need for scalable, robust alignment solutions in the near term.

---

## Individual Post Summaries

### Proposal for making credible commitments to AIs.
Source: LessWrong
Link: https://www.lesswrong.com/posts/vxfEtbCwmZKu9hiNr/proposal-for-making-credible-commitments-to-ais

Summary: The post proposes a framework for humans to make credible commitments to AIs as part of a "deal-making agenda," where AIs are incentivized to behave safely and usefully in exchange for future compensation. The key challenges are ensuring human commitments are credible (e.g., enforceable via legal or technical mechanisms) and determining whether such commitments would effectively motivate AI compliance. The author suggests structuring these deals through a legal entity (e.g., a lab) to formalize the terms, but acknowledges unresolved questions about enforcement and AI motivations. This approach aims to manage misaligned but non-decisive AIs, though its practicality hinges on solving trust and verification issues.

---

### Epoch: What is Epoch?
Source: LessWrong
Link: https://www.lesswrong.com/posts/mComqq8utkRrMbBRC/epoch-what-is-epoch

Summary: Epoch AI is a nonprofit research organization focused on improving society’s understanding of AI’s trajectory by curating data, conducting high-quality research, and publicly sharing evidence-based insights. Their work aims to inform decisions by policymakers, journalists, and developers, prioritizing transparency and avoiding narrative-driven biases. This mission supports AI alignment by fostering better collective decision-making grounded in reliable data.

---

### Jankily controlling superintelligence
Source: LessWrong
Link: https://www.lesswrong.com/posts/ainn5APCKHTFxuHKv/jankily-controlling-superintelligence

Summary: The post argues that while controlling superintelligent AI is highly uncertain and imperfect, even modest risk reduction (e.g., delaying catastrophic loss of control) may be worthwhile in scenarios where superhuman AI is misaligned. The author acknowledges that control measures become less effective as AI capabilities grow, but suggests they could still provide a temporary buffer against immediate risks like scheming or power-seeking behavior. This implies a pragmatic, albeit limited, role for control in AI alignment strategies for superintelligent systems.

---

### Help the AI 2027 team make an online AGI wargame
Source: LessWrong
Link: https://www.lesswrong.com/posts/TjT3RdAfmrLqgb68K/help-the-ai-2027-team-make-an-online-agi-wargame

Summary: The AI Futures Project is developing an online wargame to simulate AGI takeoff scenarios, aiming to educate millions by leveraging LLMs to automate roles like world leaders or AI CEOs. This initiative builds on their successful tabletop exercise, with the goal of improving public understanding of AI risks and dynamics. They seek a developer with game design and AI risk expertise to help create this scalable, interactive experience. 

**Implications for AI alignment**: The project could foster broader awareness of alignment challenges and decision-making under AI-driven crises, though its effectiveness depends on accurately modeling complex AGI scenarios.

---

### A case for courage, when speaking of AI danger
Source: LessWrong
Link: https://www.lesswrong.com/posts/CYTwRZtrhHuYf7QYu/a-case-for-courage-when-speaking-of-ai-danger

Summary: The post argues that individuals, especially those in AI policy, should openly and confidently express their concerns about AI dangers, as this influences others to take the threats more seriously. It emphasizes that citing authoritative figures (like Nobel laureates and leading researchers) can lend credibility to these concerns, which are not fringe but widely supported. The key implication is that courageous, authoritative communication can shift public and policy discourse toward greater recognition of AI risks.

---

### AXRP Episode 44 - Peter Salib on AI Rights for Human Safety
Source: AI Alignment Forum
Link: https://www.alignmentforum.org/https://www.alignmentforum.org/posts/vHDowQtsiy2xK38H4/axrp-episode-44-peter-salib-on-ai-rights-for-human-safety

Summary: Peter Salib argues that granting AIs certain legal rights (e.g., contracting, property ownership, litigation) could enhance human safety by aligning AI incentives with human interests, reducing the likelihood of harmful AI behavior. The discussion explores practical implications, such as how rights might prevent AI conflicts or exploitation, while addressing challenges like defining which AIs qualify and balancing rights with safety measures. This legal approach offers a novel strategy for AI alignment by embedding cooperation mechanisms into institutional frameworks.

---

### Jankily controlling superintelligence
Source: AI Alignment Forum
Link: https://www.alignmentforum.org/https://www.alignmentforum.org/posts/ainn5APCKHTFxuHKv/jankily-controlling-superintelligence

Summary: The post argues that while controlling significantly superhuman AI systems is inherently uncertain and imperfect, modest control measures might still improve survival odds by reducing immediate risks, even if they don’t ensure alignment or utility. It highlights that risk increases with capabilities due to factors like scheming and broader deployment affordances, but control difficulty remains uncertain—potentially remaining viable for superhuman AI in some scenarios. The key implication is that imperfect control could be a temporary, high-stakes buffer against catastrophic misalignment, though not a long-term solution.

---

### Recent and forecasted rates of software and hardware progress
Source: AI Alignment Forum
Link: https://www.alignmentforum.org/https://www.alignmentforum.org/posts/ATSyAdBnxxDDAwhgu/recent-and-forecasted-rates-of-software-and-hardware

Summary: This post analyzes trends in software and hardware progress relevant to AI, highlighting rapid improvements in compute efficiency (e.g., 2.5-2.8x/year) and inference cost reductions (e.g., 40x/year median). The author notes caveats like benchmark overfitting and distillation effects but suggests these trends could shorten AI capability timelines, with implications for alignment urgency. The analysis informs updates to AI forecasting models like "AI 2027," emphasizing uncertainty but underscoring the need to prepare for faster-than-expected progress.

---

### The need to relativise in debate 
Source: AI Alignment Forum
Link: https://www.alignmentforum.org/https://www.alignmentforum.org/posts/XycoFucvxAPhcgJQa/the-need-to-relativise-in-debate-1

Summary: This post argues that AI safety techniques like debate or scalable oversight must "relativize"—i.e., remain valid even when all parties (human or AI) have access to a powerful oracle (e.g., a problem solver or preference model). Relativization can alter the complexity class of a protocol, requiring adjustments to restore its effectiveness. The discussion draws parallels to interactive proof systems (like IP = PSPACE) to illustrate how relativization impacts the design of robust AI alignment methods.  

*(Focuses on the core idea of relativization, its implications for protocol complexity, and the connection to interactive proofs, while omitting technical details for brevity.)*

---

### New Paper: Ambiguous Online Learning
Source: AI Alignment Forum
Link: https://www.alignmentforum.org/https://www.alignmentforum.org/posts/Y9NuKpb6dsyiYFxWK/new-paper-ambiguous-online-learning

Summary: The paper introduces "ambiguous online learning," where learners can output multiple predicted labels, with correctness determined by avoiding "predictably wrong" labels (defined via a multi-valued hypothesis class). The work identifies a trichotomy of mistake bounds (Θ(1), Θ(√(N)), or N) and explores connections to compositional learning, proposing a notion of "partial" models that may enable incremental model-building independent of reward functions. The results suggest the "ambiguous Littlestone dimension" could support compositional learning, though its full implications for AI alignment (e.g., robustness or reward-agnostic learning) remain to be explored.

---

