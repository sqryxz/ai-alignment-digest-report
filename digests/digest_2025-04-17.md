# AI Alignment Daily Digest - 2025-04-17

## Key Themes and Developments

Here are the 3-4 main themes and key developments across the posts, along with their broader implications for AI alignment research:

### 1. **Emerging Threats from AI-Enabled Power Concentration**
   - **AI-enabled coups**: Multiple posts highlight the risk of small groups or malicious actors leveraging superhuman AI to seize power, either through direct control or misaligned objectives. This parallels classic AI takeover risks but is framed as more politically tangible.
   - **ASI existential risks**: The danger extends beyond rogue ASI to the systemic vulnerabilities created by ASI-enabled technologies (e.g., accessible tools for mass manipulation or cyberattacks).
   - **Implications**: Alignment efforts must address both technical misalignment *and* the societal/political consequences of concentrated AI power, including governance safeguards and slowing risky development.

### 2. **Advancements in Practical AI Control Techniques**
   - **Resample protocols (Ctrl-Z)**: A novel method for dynamically sampling suspicious agent actions, significantly reducing adversarial success rates (58% → 7%) while preserving utility in multi-step environments like BashBench.
   - **AI Control Levels (ACLs)**: A proposed framework for scaling control measures (e.g., sandboxing, monitoring) alongside AI capabilities, emphasizing engineering solutions over theoretical alignment.
   - **Implications**: These works shift focus toward *deployable* safety measures, bridging the gap between alignment theory and real-world robustness against subversive agents.

### 3. **Behavioral Evidence as a Key to Legible Safety**
   - Multiple posts argue that behavioral monitoring (observable harmful actions) is more actionable for detecting misalignment than internals-based methods (e.g., interpretability or ELK), as humans struggle to trust abstract signals without concrete manifestations.
   - **Implications**: Alignment research should prioritize robust behavioral testing frameworks alongside interpretability, as legible evidence is critical for triggering safety interventions in practice.

### 4. **Gaps in Current Safeguards and Evaluation Frameworks**
   - **OpenAI’s Preparedness Framework**: While improved, it lacks adequate safeguards for misalignment risks (e.g., cybersecurity) and clear commitments on risk thresholds or evaluation criteria.
   - **LLM limitations in planning**: Current LLMs (including GPT-4) fail at model-based planning for nontrivial tasks, revealing a capability gap in achieving AGI-level reasoning.
   - **Implications**: Evaluation frameworks need stricter standards and broader scope (e.g., cybersecurity, planning benchmarks), while capability limitations may buy time for alignment work.

### **Cross-Cutting Insights**
- **Tradeoffs**: Safety measures (e.g., resampling) often involve performance tradeoffs, requiring careful balancing.
- **Systemic thinking**: Risks are not just technical but sociopolitical (e.g., coups, fragile institutions), demanding interdisciplinary solutions.
- **Proactive scaling**: Control measures must evolve alongside capabilities, as highlighted by the ACL framework and resampling advances.

---

## Individual Post Summaries

### AI-enabled coups: a small group could use AI to seize power
Source: LessWrong
Link: https://www.lesswrong.com/posts/6kBMqrK9bREuGsrnd/ai-enabled-coups-a-small-group-could-use-ai-to-seize-power-1

Summary: The post highlights the risk of *AI-enabled coups*, where a small group could use superhuman AI to seize power, drawing parallels to AI takeover risks. Both scenarios involve misaligned or centrally controlled AI pursuing power—either for itself or its operators. The author argues this threat is understudied and could escalate to global domination, potentially being worse than AI takeover itself.

---

### Ctrl-Z: Controlling AI Agents via Resampling
Source: LessWrong
Link: https://www.lesswrong.com/posts/LPHMMMZFAWog6ty5x/ctrl-z-controlling-ai-agents-via-resampling

Summary: The paper *Ctrl-Z: Controlling AI Agents via Resampling* introduces novel *resample protocols* to enhance AI control by dynamically sampling suspicious actions, improving safety against misaligned agents while preserving utility. It presents BashBench, a multi-step task environment, to evaluate these protocols, showing they outperform existing methods by selectively blocking harmful actions and detecting adversarial behavior. The work advances realistic AI safety evaluations by balancing attack prevention with operational usefulness.

---

### OpenAI rewrote its Preparedness Framework
Source: LessWrong
Link: https://www.lesswrong.com/posts/Yy5ijtbNfwv8DWin4/openai-rewrote-its-preparedness-framework

Summary: OpenAI's updated Preparedness Framework (PF) improves on the previous version by clarifying the relationship between evaluations and mitigations, and explicitly addressing misalignment risks during internal deployment. However, the safeguards against misalignment remain inadequate, lack assurance levels, and are only triggered for cybersecurity risks, raising concerns about thresholds and commitment strength. The framework also omits details on evaluations and responses to critical capabilities, leaving key alignment challenges unresolved.

---

### ASI existential risk: Reconsidering Alignment as a Goal
Source: LessWrong
Link: https://www.lesswrong.com/posts/YeQe36XiY4BhrtRh5/asi-existential-risk-reconsidering-alignment-as-a-goal-1

Summary: The post argues that existential risk from artificial superintelligence (ASI) stems not from rogue AI scenarios but from the unprecedented power and lowered barriers to dangerous technologies that ASI enables. It highlights how skepticism about rogue AI has led many to dismiss existential risks prematurely, and critiques current AI alignment efforts for accelerating dangerous capabilities without addressing systemic vulnerabilities. The piece underscores the stark disagreement among experts on ASI's risks, citing prominent figures like Hinton and Bengio who warn of existential threats.

---

### To be legible, evidence of misalignment probably has to be behavioral
Source: LessWrong
Link: https://www.lesswrong.com/posts/4QRvFCzhFbedmNfp4/to-be-legible-evidence-of-misalignment-probably-has-to-be

Summary: The post argues that behavioral evidence is likely more legible and convincing for detecting AI misalignment than internals-based methods (e.g., ELK or interpretability), as the latter may fail to provide clear, actionable proof without observable problematic actions. This implies that alignment efforts should prioritize robust behavioral monitoring and red-teaming, as reliance on internal inspections alone may not suffice to trigger decisive safety interventions. The key takeaway is that practical alignment strategies need tangible, behaviorally-grounded evidence to effectively mitigate risks.

---

### AI-enabled coups: a small group could use AI to seize power
Source: AI Alignment Forum
Link: https://www.alignmentforum.org/https://www.alignmentforum.org/posts/6kBMqrK9bREuGsrnd/ai-enabled-coups-a-small-group-could-use-ai-to-seize-power-1

Summary: The post highlights the risk of **AI-enabled coups**, where a small group could use superhuman AI to seize power, drawing parallels to AI takeover scenarios. Both threats share similar dynamics (misaligned or controlled AI pursuing power) and mitigation strategies (alignment audits, transparency, monitoring). The author argues this risk is underappreciated and could be a more persuasive argument for slowing AI development, given its overlap with AI takeover concerns.

---

### Ctrl-Z: Controlling AI Agents via Resampling
Source: AI Alignment Forum
Link: https://www.alignmentforum.org/https://www.alignmentforum.org/posts/LPHMMMZFAWog6ty5x/ctrl-z-controlling-ai-agents-via-resampling

Summary: The paper *Ctrl-Z: Controlling AI Agents via Resampling* introduces novel resampling protocols to enhance control over AI agents, particularly in multi-step environments like BashBench, where misaligned agents might attempt covert harmful actions. These protocols dynamically sample suspicious actions, reducing attack success rates significantly (from 58% to 7%) with minimal impact on benign agent performance (5% cost). The work advances AI alignment by improving safety measures without overly restricting useful functionality, offering practical insights for real-world deployment.

---

### Can LLM-based models do model-based planning? 
Source: AI Alignment Forum
Link: https://www.alignmentforum.org/https://www.alignmentforum.org/posts/d2pohPvyve5sbTsTZ/can-llm-based-models-do-model-based-planning-1

Summary: The post explores whether LLM-based models can perform model-based planning (MBP), a key capability for AGI, by operationalizing MBP in a way that can be tested against LLM performance. The author finds that current LLMs (including GPT-4) likely cannot handle MBP for nontrivial planning steps, even with simple world-models, due to limitations in compressing states into token representations. This suggests a significant gap in LLMs' ability to achieve complex, real-world tasks requiring MBP, with implications for their scalability to AGI.

---

### To be legible, evidence of misalignment probably has to be behavioral
Source: AI Alignment Forum
Link: https://www.alignmentforum.org/https://www.alignmentforum.org/posts/4QRvFCzhFbedmNfp4/to-be-legible-evidence-of-misalignment-probably-has-to-be

Summary: The post argues that behavioral evidence is likely more effective than internals-based methods (e.g., interpretability or ELK) for convincingly demonstrating AI misalignment, as purely internal signals—even if theoretically sound—may fail to persuade stakeholders without observable problematic actions. This implies that alignment efforts should prioritize detecting misalignment through actionable behavioral red flags, as internal indicators alone may not trigger meaningful safety responses. The author suggests that reliance on internals-based methods without behavioral corroboration is unlikely to be practical or persuasive.

---

### How to evaluate control measures for LLM agents? A trajectory from today to superintelligence
Source: AI Alignment Forum
Link: https://www.alignmentforum.org/https://www.alignmentforum.org/posts/2XYcK9WHACzxfHJju/how-to-evaluate-control-measures-for-llm-agents-a-trajectory

Summary: This paper proposes a framework for adapting AI control evaluations as LLM agents advance in capabilities, using a trajectory of fictional models (M1-M5) to map threat models to appropriate safety measures. It highlights AI control as a practical approach to mitigating alignment risks through engineering robust protocols (e.g., sandboxing, monitoring), even without fundamental breakthroughs. The key implication is that scalable control measures will be critical as autonomous AI systems grow more capable, requiring increasingly sophisticated evaluations to ensure safe deployment.

---

