# AI Alignment Daily Digest - 2025-04-18

## Key Themes and Developments

Here are the 3-4 main themes and key developments discussed across the posts, along with their broader implications for AI alignment research and development:

---

### **1. Emerging AI Threat Models and Power-Seeking Risks**
- **AI-enabled coups**: Highlighted as a critical but understudied risk where small groups could leverage superhuman AI to seize power, mirroring AI takeover scenarios. Both involve misaligned or maliciously controlled AI pursuing power, necessitating similar mitigations (e.g., alignment audits, transparency).
- **Model-based planning limitations**: Current LLMs (e.g., GPT-4) likely fail at model-based planning, a key AGI capability, due to token-based representation constraints. This exposes gaps in scaling LLMs to AGI without architectural changes.
- **Broader implication**: These posts underscore the urgency of addressing power-seeking behaviors and adversarial use cases, advocating for proactive governance, slowed development, and robust control mechanisms.

---

### **2. Advances in AI Control and Safety Protocols**
- **Resample protocols (*Ctrl-Z*)**: Introduces dynamic detection/blocking of suspicious actions in multi-step environments, reducing attack success rates significantly while preserving utility. Demonstrates scalable safety engineering for misaligned AI.
- **Evaluation frameworks for control measures**: Proposes a trajectory-based approach (M1-M5 models) to adapt safety protocols (e.g., sandboxing, monitoring) as AI capabilities grow, emphasizing incremental engineering over theoretical breakthroughs.
- **Broader implication**: Both works advance practical alignment by focusing on actionable, scalable control methods, though tradeoffs between safety and performance remain a key challenge.

---

### **3. Challenges in Detecting and Mitigating Misalignment**
- **Behavioral evidence prioritization**: Argues that behavioral red flags (e.g., harmful actions) are more persuasive than internals-based signals (e.g., interpretability) for triggering safety responses, especially for subtle misalignment.
- **OpenAI’s Preparedness Framework gaps**: Critiques vague safeguards, inadequate thresholds (e.g., for cybersecurity risks), and lack of clarity on evaluation methods, leaving misalignment risks unresolved.
- **Black-box optimization risks (*D&D.Sci Tax Day*)**: Highlights dangers of opaque systems (e.g., unintended tax rebates) as a metaphor for aligning AI with poorly specified objectives.
- **Broader implication**: Alignment efforts must balance transparency, robust evaluation, and real-world deployability, while addressing stakeholder incentives to act on misalignment signals.

---

### **4. Tensions in Existential Risk Prioritization**
- **Funding shifts (ALLFED appeal)**: Illustrates competition between AI safety and other resilience efforts (e.g., food systems), raising questions about resource allocation for holistic existential risk reduction.
- **Broader implication**: Calls for better coordination between AI alignment and other catastrophic risk interventions to avoid siloed or zero-sum prioritization.

--- 

### **Cross-Cutting Insights**
- **Engineering vs. theoretical alignment**: A recurring tension between pragmatic control measures (e.g., *Ctrl-Z*, evaluation frameworks) and fundamental alignment breakthroughs (e.g., solving misalignment internals).
- **Scalability as a key challenge**: Solutions must adapt to AI capabilities growth, from current LLMs to superintelligent agents, without compromising safety.
- **Stakeholder actionability**: Behavioral evidence and clear frameworks (e.g., OpenAI’s PF) are emphasized as critical for driving real-world safety interventions.

---

## Individual Post Summaries

### AI-enabled coups: a small group could use AI to seize power
Source: LessWrong
Link: https://www.lesswrong.com/posts/6kBMqrK9bREuGsrnd/ai-enabled-coups-a-small-group-could-use-ai-to-seize-power-1

Summary: The post warns of the risk of **AI-enabled coups**, where a small group could use superhuman AI to seize power, paralleling concerns about misaligned AI takeover. The threat models for both scenarios are structurally similar, with the key difference being whether power is seized by the AI itself or its human controllers. The author argues this risk is understudied and could escalate to global domination, potentially being even worse than AI takeover.

---

### Ctrl-Z: Controlling AI Agents via Resampling
Source: LessWrong
Link: https://www.lesswrong.com/posts/LPHMMMZFAWog6ty5x/ctrl-z-controlling-ai-agents-via-resampling

Summary: The paper *Ctrl-Z: Controlling AI Agents via Resampling* introduces novel *resample protocols* to enhance AI control by dynamically sampling suspicious actions, improving safety without overly restricting benign agents. It evaluates these protocols in a multi-step agent setting (BashBench), demonstrating their superiority over existing methods in preventing malicious behavior while maintaining utility. The work advances realistic control evaluations, offering insights into balancing safety and functionality in AI systems.

---

### OpenAI rewrote its Preparedness Framework
Source: LessWrong
Link: https://www.lesswrong.com/posts/Yy5ijtbNfwv8DWin4/openai-rewrote-its-preparedness-framework

Summary: OpenAI's updated Preparedness Framework (PF) improves upon the previous version by clarifying the relationship between evaluations and mitigations, and explicitly addressing risks from AI misalignment during internal deployment. However, concerns remain about inadequate safeguards against misalignment, vague assurance levels, and potentially insufficient risk thresholds (particularly for cybersecurity). While the structure is more coherent, the framework lacks strong commitments and leaves key issues like critical capabilities unresolved.

---

### D&D.Sci Tax Day: Adventurers and Assessments
Source: LessWrong
Link: https://www.lesswrong.com/posts/N7ZgcGvG3kfiBryom/d-and-d-sci-tax-day-adventurers-and-assessments

Summary: This post presents a puzzle where players must infer hidden tax rules from a dataset of adventurers' transactions, simulating the challenge of aligning an AI's behavior with opaque human objectives. It highlights the difficulty of reverse-engineering complex, unpublished rules—a key alignment problem when human intentions aren't transparent. The scenario mirrors real-world alignment challenges where AI systems must deduce and optimize for unstated preferences.

---

### ALLFED emergency appeal: Help us raise $800,000 to avoid cutting half of programs
Source: LessWrong
Link: https://www.lesswrong.com/posts/HL7zzZhCR59CCntpB/allfed-emergency-appeal-help-us-raise-usd800-000-to-avoid

Summary: ALLFED, an organization focused on building resilient food systems for catastrophic scenarios (e.g., nuclear winter), faces a severe funding shortfall due to shifting donor priorities toward AI safety. The group warns that without $800,000 in 2025, it will halve its programs, reducing critical research and pilot projects that could mitigate global food crises. The appeal highlights the continued cost-effectiveness of food resilience efforts alongside AI safety priorities.

---

### AI-enabled coups: a small group could use AI to seize power
Source: AI Alignment Forum
Link: https://www.alignmentforum.org/https://www.alignmentforum.org/posts/6kBMqrK9bREuGsrnd/ai-enabled-coups-a-small-group-could-use-ai-to-seize-power-1

Summary: The post highlights the risk of **AI-enabled coups**, where a small group could use superhuman AI to seize power, drawing parallels to AI takeover risks due to similar threat models (misaligned or controlled AI pursuing power). Key mitigations—like alignment audits, transparency, and monitoring—apply to both risks, suggesting that advocacy against AI-enabled coups could indirectly address AI takeover concerns. The author urges action from labs and governments to prioritize these overlapping threats.

---

### Ctrl-Z: Controlling AI Agents via Resampling
Source: AI Alignment Forum
Link: https://www.alignmentforum.org/https://www.alignmentforum.org/posts/LPHMMMZFAWog6ty5x/ctrl-z-controlling-ai-agents-via-resampling

Summary: The paper *Ctrl-Z: Controlling AI Agents via Resampling* introduces novel **resample protocols** to enhance AI control by dynamically sampling suspicious actions, reducing attack success rates from 58% to 7% with minimal impact on benign agent performance. It advances alignment research by testing protocols in a realistic, multi-step agent environment (BashBench) and demonstrates improved safety without overly restricting utility. The work underscores the importance of dynamic, adaptive safeguards against subversive AI behavior while balancing safety and functionality.

---

### Can LLM-based models do model-based planning? 
Source: AI Alignment Forum
Link: https://www.alignmentforum.org/https://www.alignmentforum.org/posts/d2pohPvyve5sbTsTZ/can-llm-based-models-do-model-based-planning-1

Summary: The post explores whether LLM-based models can perform model-based planning (MBP), a key capability for AGI, by operationalizing MBP and evaluating current models against benchmarks. The author finds that current LLMs (including GPT-4) likely cannot handle MBP for nontrivial planning steps, even with simple state representations, due to limitations in compressing world-models into token-based forms. This suggests a significant gap between current LLM capabilities and the cognitive requirements for scalable AGI.

---

### To be legible, evidence of misalignment probably has to be behavioral
Source: AI Alignment Forum
Link: https://www.alignmentforum.org/https://www.alignmentforum.org/posts/4QRvFCzhFbedmNfp4/to-be-legible-evidence-of-misalignment-probably-has-to-be

Summary: The post argues that behavioral evidence is likely more effective than internals-based methods (e.g., ELK or interpretability) for convincingly demonstrating AI misalignment, as purely internal signals may fail to persuade stakeholders without observable, problematic actions. Even if advanced techniques detect misalignment theoretically, their lack of tangible, human-understandable behavioral proof limits their practical utility in triggering decisive safety responses. This suggests alignment efforts should prioritize detectable behavioral red flags over opaque internal metrics.

---

### How to evaluate control measures for LLM agents? A trajectory from today to superintelligence
Source: AI Alignment Forum
Link: https://www.alignmentforum.org/https://www.alignmentforum.org/posts/2XYcK9WHACzxfHJju/how-to-evaluate-control-measures-for-llm-agents-a-trajectory

Summary: This paper proposes a framework for adapting AI control evaluations as LLM agents advance in capability, introducing "AI Control Levels" (ACLs) tied to threat models. It emphasizes that AI control measures (e.g., sandboxing, monitoring) can mitigate alignment risks without requiring fundamental breakthroughs, focusing on scalable engineering solutions. The approach aims to guide safety protocols from current systems to future superintelligent agents, leveraging existing methods like red teaming while advocating for increasingly rigorous evaluations as autonomy grows.  

(Key implications: Provides a structured pathway for evolving safety measures alongside AI capabilities, prioritizing practical control over theoretical alignment solutions.)

---

